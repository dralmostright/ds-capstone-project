---
title: "Coursera DS Capstone Project"
author: "Suman Adhikari"
date: "October 13, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary
Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. Hence in this Coursera Capstone project I am motivated to address the problem by starting with analyzing a large corpus of text documents to discover the structure in the data and how words are put together, then sampling and building from a predictive text model.

This milestone report is part of Coursera Data Science Specialization: Capstone project which delineate major features of the training data with exploratory data analysis and summarizes plans for creating the predictive model.

**Source code of each user define function and plots used in Code chunks are set to echo=FALSE , hence can be viewed in [Appendix - I](#appendix---i-user-defined-functions) and [Appendix - II](#appendix--ii-plots).**

## Load the required libraries
The necessary packages for processing documents/corpus, exploratory analysis and bulding model are loaded.
```{r, message=FALSE, results='hide'}
libs <- c("ggplot2","tm","SnowballC","reshape2","RWeka","dplyr")
lapply(libs ,require, character.only=TRUE)
```

## Load the Data
Data used for this Capstone Project was obtained from the below URL:

* [Data Set](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

The data had been already download to working directory and extracted. I will be using the English Language documents for our analysis as other language are also available.

```{r}
## List the documents in our working directory
dir("/R/workspace/capstoneProject/dataset/final/en_US")
```

## Exploratory Data Analysis-I and Sampling Data 
Before starting to clean data, lets first analyze the few attributes of the data i.e size, no. of lines etc.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
fileInfo <- function(inputFile,directory) {
## Set the working directory
setwd(directory)
## Get the size of the file in MB
fSize <- file.info(inputFile)[1]/(1024*1024)
## Open file for reading in text mode
conn <- file(inputFile, open="r")
## Read the lines in memory
lines <- readLines(conn)
## Calculate the total human readable charactes
nChars <- lapply(lines, nchar)
## Get the maximum value for nChars
maxChars <- nChars[which.max(nChars)][[1]]
## Get the Total words for the file
nWords <- sum(sapply(strsplit(lines, "\\s+"),length))
## Close connection
close(conn)
## Retrun the summary
return(c(inputFile, round(fSize,2), length(lines), maxChars, nWords))
}

rawCorpusSummary <- function(directory){
fList <- list.files(directory)
summaryInfo <- lapply(fList,fileInfo,directory=directory)
summaryInfo <- data.frame(matrix(unlist(summaryInfo),length(fList), byrow=T))
colnames(summaryInfo) <- c("File.Name", "Size.MB", "Total.Lines", "Max.Line.Length", "No.Words")
summaryInfo
}
```
```{r, message=FALSE, warning=FALSE}
rawCorpusSummary("/R/workspace/capstoneProject/dataset/final/en_US")
```

The documents we are going to use are big, as we obeserved from *summaryInfo*. Hence processing of those whole documents in normal PC takes siginificant time and memory. Hence to I will sample the document with 1%~10% of original and proceed. I will be using random sampling algorithm to get sample document.
```{r, message=FALSE, warning=FALSE, echo=FALSE}
## selectSample take the document checks its total line size & select the sample based on no of lines
## Set the seed for reproducibility
set.seed(545)
## Function to get the sample lines from docs
selectSample <- function(file, sampleSize, samplePath){
## Open file for reading in text mode
conn <- file(file, open="r")
## Read the lines in memory
lines <- readLines(conn)
## Get no of lines for the file 
fileLength <- length(lines)
## Get the sample lines size based on the total lines on file
sampleSize <- round((sampleSize/100) * fileLength)
## Get the random Indexes
randIndex <- sample(1:fileLength, sampleSize, replace=TRUE)
## Select the lines corresponding to random index
sampleFile <- lines[randIndex]

## Prepare the sampleFile to write back to disk
## Get the name of File
samFileName <- paste0(samplePath, tail(strsplit(file,"/")[[1]],1))
# Check sample file path
sampleTemPath <- gsub("sam_","", samplePath)

# If directory exits check also if file exits & If file exits remove the file
if(dir.exists(sampleTemPath)){
	if(file.exists(samFileName)){
	file.remove(samFileName)
	}
}
# Create the sample path directory
else {
	dir.create(sampleTemPath, showWarnings = TRUE, recursive = TRUE, mode = "0775")
}

# Prepare connection
samFileConn <- file(samFileName, "w+")
# write to disk
writeLines(sampleFile, samFileConn)

## Close all file connection
close(samFileConn)
close(conn)
}

## Function to get sample docs
makeSample <- function(directory,samplePath, sampleSize){

# Get the list of the files 
fList <- list.files(directory, full.names = TRUE)

# Select the files from the directory, get sample size and save to disk 
samlist <- lapply(fList, selectSample, sampleSize=sampleSize,samplePath=samplePath)
print("Sample docs are processed and wrote to disk.")
}
```

```{r, message=FALSE, warning=FALSE}
samplePath <- "/R/workspace/capstoneProject/dataset/final/sample/sam_"
sourcePath <- "/R/workspace/capstoneProject/dataset/final/en_US"
#makeSample(sourcePath, samplePath, 1)
corpusPath <- gsub("sam_","", samplePath)
```

The samples are processed and wrote back to disk using function *makeSample*. Now, lets look how our sample is in terms of size, word lenght etc.
```{r, message=FALSE, warning=FALSE}
rawCorpusSummary(corpusPath)
corpus.docs <- VCorpus(DirSource(corpusPath))
```

## Getting and Cleaning Data

The documents that have been used are from various sources like twitter, blog and news. The documents may consists of various flaws and profanity like unwanted symbols, contractions, stop words, bad words. Hence our first step should be to handle them properly. The tasks I am proforming as part of text mining are listed below in bold:

 * **Changing the case of corpus**
 * **Removing the contractions**
 * **Removing numbers and punctuations**
 * Removing stop words and bad words 
 * **Removing unwanted white spaces**
 * **Stemming the corpus**

I am using tm package and its functions to clean the data. 



### Change the case of words to lower
As R is case sensitive, we need to convert the case of whole corpus as *Case* is not equal to *case*.
However, there is a tolower transformation, it is not a part of the standard tm transformations. For this reason, I have to convert tolower into a transformation that can handle a corpus object properly by using content_transformer.

```{r echo=FALSE}
toLower <- function(corpus){
  corpus <- tm_map(corpus,content_transformer(tolower))
  return(corpus)
}
```



### Handling contractions
Words like I'll, I'm i.e contractions are heavily used in english as they represent two words. Hence to fix such condition I have developed a function which splits the words 'i'll' to 'i will', 'i'am' to 'i am' etc.
```{r echo=FALSE}
mapFun <- content_transformer(function(x, src, dest) {return (gsub(src, dest, x))})
completeCase <- function(inputCorpus){
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'ll", dest=" will")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="can't", dest="cannot")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="won't", dest="will not")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'ve", dest=" have")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="n't", dest=" not")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'m", dest=" am")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="`m", dest=" am")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'d", dest=" had")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'s", dest=" is")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'re", dest=" are");
  return (inputCorpus)
}
```

### Stemming the corpus
As the corpus is huge and contain many words that have a common root for example: offer, offered and offering.  Hence we will stem the corpus using tm_map function. 

### Removing bad words and stop words.
We are imitating natural language, so i think its better to keep the stop words. As removing stop word may make our model more worse.

### Remove extraneous whitespaces
All extraneous whitespaces are removed using the stripWhitespace transformation:

As we have our sample corpus and developed some functions to polish our corpus, its time to use them in our corpus to clean and polish it.
```{r}
corpus.docs <- toLower(corpus.docs)
corpus.docs <- completeCase(corpus.docs)
corpus.docs <- tm_map(corpus.docs, removeNumbers)
corpus.docs <- tm_map(corpus.docs, removePunctuation)
corpus.docs <- tm_map(corpus.docs, stripWhitespace)
corpus.docs <- tm_map(corpus.docs, stemDocument)
```

## Exploratory Data Analysis - II
Now I have developed our tidy and clean corpus. Next will be exploring the corpus for distribution of terms and their frequency, n-grams and their frequency. I will use Term Document Matrix for exploring corpus i.e rows represend the document and columns represent terms that appeared in the matrix.

### Creating Term Document matrix
With preprocessing now I will change the text to quantitative for further analysis using Term Document Matrix.

```{r , echo=FALSE}
## ngramTDM function takes input as corpus and creates ngram Term Document Matrix
## start specifies the starting point for ngarm 
## stop specifies the ending point for ngram
## finally retruns term document matrix from start to stop
ngramTDM <- function(corpus, start=NULL, stop=NULL){
  ## Handling the start and stop values
  if(is.null(start) & is.null(stop)){
    start <- 1
    stop <- 1
  }
  else if (is.null(start) & !is.null(stop)){
    start <- 1
  }
  else if (!is.null(start) & is.null(stop)){
    stop <- start
  }
  else {
	if(stop < start){
		print("Stop value is lesser than start.")
	}
  }
  
  ## Creating tokenizer handle
  ngramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=start, max=stop))
  
  ## Create term document matrix 
  tdm <- TermDocumentMatrix(corpus, control = list(tokenize = ngramTokenizer))
  return(tdm)
}
```

After creating Term document matrix using function *ngramTDM* I am interested in Term frequency for each ngarm TDM. I will use getTermFreq user define function takes the ngram TDM and retruns Term frequency matrix.
```{r, echo=FALSE}
getTermFreq <- function(tdm, threshold=NULL){
	freqTerm <- rowSums(as.matrix(tdm))
	oraIndex <- order(freqTerm, decreasing = TRUE)
	if(is.null(threshold)){
		data <- melt(freqTerm[oraIndex])
	}
	else {
	    data <- melt(head(freqTerm[oraIndex], threshold))
	}
	remove(freqTerm)
	gc()
	data$term <- dimnames(data)[[1]]
	return(data)
}
```

Let's create TDM inspect how it looks like.
```{r}
corpus.uniTDM <- ngramTDM(corpus.docs)
corpus.biTDM <- ngramTDM(corpus.docs,2)
corpus.triTDM <- ngramTDM(corpus.docs,3)
corpus.fourTDM <- ngramTDM(corpus.docs,4)
```

### Mining the TDM
I have transformed text to a mathemetical object and now I can analyze it using quantitative measures.
For example, to get the frequency of occurrence of each word in the corpus, I simply sum over all rows to give column sums.

First let's visualize unigram in plot.

```{r, echo=FALSE}
uniTFM <- getTermFreq(corpus.uniTDM, 20)

p <- ggplot(uniTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("Unigram Term Frequency in Corpus")
p <- p + xlab("Unigram Terms in Corpus") + ggtitle("Unigram Term frequency VS Term in Corpus")
p
```

It can be clearly witnessed form above plot that most frequent words are stop words. As stated earlier I have not removed stop words as we are imitating the NLP. Hence removing stop word may make our model more worse.

Next lets visualize bigram, trigram and 4-gram term document matrix.
```{r, echo= FALSE}
biTFM <- getTermFreq(corpus.biTDM, 20)
p <- ggplot(biTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("Bigram Term Frequency in Corpus")
p <- p + xlab("Bigram Terms in Corpus") + ggtitle("Bigram Term frequency VS Term in Corpus")
p


triTFM <- getTermFreq(corpus.triTDM, 20)
p <- ggplot(triTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("Trigram Term Frequency in Corpus")
p <- p + xlab("Trigram Terms in Corpus") + ggtitle("Trigram Term frequency VS Term in Corpus")
p

fourTFM <- getTermFreq(corpus.fourTDM, 20)
p <- ggplot(fourTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("4gram Term Frequency in Corpus")
p <- p + xlab("4gram Terms in Corpus") + ggtitle("4gram Term frequency VS Term in Corpus")
p
```

## Word coverage
#### How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
To find the solution to this question, I will iterate over our sample data's Frequency Term Matrix and find the required word instances to coverage 50 % and 90 % respectively using function calFreqWordInstance. All user defined functions definition can be found on Appendix - I.
```{r, echo=FALSE}
calFreqWordInstance <- function(tfm, threshold){
wordFreq <- 0
uniqueCount <- 0
totalWord <- sum(tfm$value)
for(i in 1:nrow(tfm)){
wordFreq <- wordFreq + tfm[i, "value"]
wordCoverage <-  round((wordFreq/totalWord) * 100, 2)
if(wordCoverage >= threshold){
  uniqueCount <- i
  return(uniqueCount)
  break
}
}
}
uniTFM <- getTermFreq(corpus.uniTDM)
```

The no. of unique words required to cover 50% and 90% of all word instance are *`r calFreqWordInstance(uniTFM, 50)`* and *`r calFreqWordInstance(uniTFM, 90)`* respectively.
With calculating the no. of unique words required to cover 50% and 90% of all word instance, lets visualize it via plot.
```{r, echo=FALSE}
wordPercent <- seq(10, 90, by=10)

totalTermCount <- sum(uniTFM$value)
wordPerValue <- 0
wordCount <- 0
j <- 1
for(i in 1:nrow(uniTFM)){
wordCount <- wordCount + uniTFM[i, "value"]
wordCoverage <- round((wordCount/totalTermCount) * 100,2)
if (wordCoverage >= wordPercent[j]){
wordPerValue[j] <- i
j <- j+ 1
}
if(j > 9){
break
}
}

coverageDF <- as.data.frame(wordPercent)
coverageDF$uniqueWords <- wordPerValue
y50 <- calFreqWordInstance(uniTFM, 50)
y90 <- calFreqWordInstance(uniTFM, 90)
p <- ggplot(coverageDF, aes(wordPercent, uniqueWords)) + geom_point(alpha=0.5) + geom_line()
p <- p + geom_point(aes(x=50, y=y50), size = 4, colour = "blue", alpha=0.2)
p <- p + geom_point(aes(x=90, y=y90), size = 4, colour = "red", alpha=0.2)
p <- p + geom_text(x=50, y=350, label=paste("Word instance required for 50% coverage:",y50), colour= "blue") 
p <- p + geom_text(x=65, y=4850, label=paste("Word instance required for 90% coverage:", y90), colour= "red")
p <- p + theme(plot.title = element_text(hjust = 0.5))
p <- p + ylab("Unique Word Count")
p <- p + xlab("No of wrods in Corpus in %") + ggtitle("Unique word Count Vs No of wrods in Corpus in %")
p
```

## Findings that amassed so far
So far there was not so amusing thing i have encountered. But some that have triggred me are:

* Tokenizer was not working with Simple Corpus. As I have created VCorpus as as.VCorpus(FileFromMemory), but the tm function tm_map was was not working. Hence, it was slowing my computation as, first I have to load the files in memory do some sampling and again right back to memory and again load it. 
* As all must have felt slowness that R is taking time to do computation, so there must be parallelism with tm package and Tokenizer.

## Questions to consider

##Get feedback
I am planning to use Markov Chain to implement my model. The algoright flow is:

* Take the n-gram input
* search the n-gram in n+1-gram
    + if no match found
      * fallback the search to search the ngram by purneing first term of n-gram input in n-gram
      * repeat step i until n-gram input is 1-gram and database gran is 2-gram 
    + if still no match found 
      + randomly select fron 1-gram and retrun word.

My search is slow when the sample size above 5%. So any techniques you can suggest like indexing, parallelizing for search so that the response time is quick. Also, further suggestion for amendment in algorithm to tune response time are welcome.

## Appendix - I (User defined functions)

1. Function to view attributes of the data 

```{r, message=FALSE, warning=FALSE, eval=FALSE}
fileInfo <- function(inputFile,directory) {
## Set the working directory
setwd(directory)
## Get the size of the file in MB
fSize <- file.info(inputFile)[1]/(1024*1024)
## Open file for reading in text mode
conn <- file(inputFile, open="r")
## Read the lines in memory
lines <- readLines(conn)
## Calculate the total human readable charactes
nChars <- lapply(lines, nchar)
## Get the maximum value for nChars
maxChars <- nChars[which.max(nChars)][[1]]
## Get the Total words for the file
nWords <- sum(sapply(strsplit(lines, "\\s+"),length))
## Close connection
close(conn)
## Retrun the summary
return(c(inputFile, round(fSize,2), length(lines), maxChars, nWords))
}


rawCorpusSummary <- function(directory){
fList <- list.files(directory)
summaryInfo <- lapply(fList,fileInfo,directory=directory)
summaryInfo <- data.frame(matrix(unlist(summaryInfo),length(fList), byrow=T))
colnames(summaryInfo) <- c("File.Name", "Size.MB", "Total.Lines", "Max.Line.Length", "No.Words")
summaryInfo
}
```

2. Function to Get the sample.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
## selectSample take the document checks its total line size & select the sample based on no of lines
## Set the seed for reproducibility
set.seed(545)
## Function to get the sample lines from docs
selectSample <- function(file, sampleSize, samplePath){
## Open file for reading in text mode
conn <- file(file, open="r")
## Read the lines in memory
lines <- readLines(conn)
## Get no of lines for the file 
fileLength <- length(lines)
## Get the sample lines size based on the total lines on file
sampleSize <- round((sampleSize/100) * fileLength)
## Get the random Indexes
randIndex <- sample(1:fileLength, sampleSize, replace=TRUE)
## Select the lines corresponding to random index
sampleFile <- lines[randIndex]

## Prepare the sampleFile to write back to disk
## Get the name of File
samFileName <- paste0(samplePath, tail(strsplit(file,"/")[[1]],1))
# Check sample file path
sampleTemPath <- gsub("sam_","", samplePath)

# If directory exits check also if file exits & If file exits remove the file
if(dir.exists(sampleTemPath)){
	if(file.exists(samFileName)){
	file.remove(samFileName)
	}
}
# Create the sample path directory
else {
	dir.create(sampleTemPath, showWarnings = TRUE, recursive = TRUE, mode = "0775")
}

# Prepare connection
samFileConn <- file(samFileName, "w+")
# write to disk
writeLines(sampleFile, samFileConn)

## Close all file connection
close(samFileConn)
close(conn)
}
```

3. Function to Sample the corpus.
```{r, eval=FALSE}
## Function to get sample docs
makeSample <- function(directory,samplePath){

# Get the list of the files 
fList <- list.files(directory, full.names = TRUE)

# Select the files from the directory, get sample size and save to disk 
samlist <- lapply(fList, selectSample, sampleSize=5,samplePath=samplePath)
print("Sample docs are processed and wrote to disk.")
}
```

4. Functions to clean the corpus.

Change the case to lower
```{r, eval=FALSE}
toLower <- function(corpus){
  corpus <- tm_map(corpus,content_transformer(tolower))
  return(corpus)
}
```

Handle contractions.
```{r, eval=FALSE}
mapFun <- content_transformer(function(x, src, dest) {return (gsub(src, dest, x))})
completeCase <- function(inputCorpus){
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'ll", dest=" will")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="can't", dest="cannot")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="won't", dest="will not")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'ve", dest=" have")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="n't", dest=" not")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'m", dest=" am")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="`m", dest=" am")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'d", dest=" had")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'s", dest=" is")
  inputCorpus <- tm_map(inputCorpus, mapFun, src="'re", dest=" are");
  return (inputCorpus)
}
```

5. Term Document Matrix
Generate ngram Term Document Matrix
```{r, eval=FALSE}
## ngramTDM function takes input as corpus and creates ngram Term Document Matrix
## start specifies the starting point for ngarm 
## stop specifies the ending point for ngram
## finally retruns term document matrix from start to stop
ngramTDM <- function(corpus, start=NULL, stop=NULL){
  ## Handling the start and stop values
  if(is.null(start) & is.null(stop)){
    start <- 1
    stop <- 1
  }
  else if (is.null(start) & !is.null(stop)){
    start <- 1
  }
  else if (!is.null(start) & is.null(stop)){
    stop <- start
  }
  else {
	if(stop < start){
		print("Stop value is lesser than start.")
	}
  }
  
  ## Creating tokenizer handle
  ngramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=start, max=stop))
  
  ## Create term document matrix 
  tdm <- TermDocumentMatrix(corpus, control = list(tokenize = ngramTokenizer))
  return(tdm)
}
```
Generate Term Frequency matrix for ngram TDM
```{r, eval=FALSE}
getTermFreq <- function(tdm, threshold=NULL){
	freqTerm <- rowSums(as.matrix(tdm))
	oraIndex <- order(freqTerm, decreasing = TRUE)
	if(is.null(threshold)){
		data <- melt(freqTerm[oraIndex])
	}
	else {
	    data <- melt(head(freqTerm[oraIndex], threshold))
	}
	remove(freqTerm)
	gc()
	data$term <- dimnames(data)[[1]]
	return(data)
}
```

6. Function to get Word frequency required to cover n% of words in corpus.
```{r, eval=FALSE}
calFreqWordInstance <- function(tfm, threshold){
wordFreq <- 0
uniqueCount <- 0
totalWord <- sum(tfm$value)
for(i in 1:nrow(tfm)){
wordFreq <- wordFreq + tfm[i, "value"]
wordCoverage <-  round((wordFreq/totalWord) * 100, 2)
if(wordCoverage >= threshold){
  uniqueCount <- i
  return(uniqueCount)
  break
}
}
}
```

7. Code to get no. of unique words required to cover 10% to 90% of all word instance.
```{r , eval=FALSE}
wordPercent <- seq(10, 90, by=10)

totalTermCount <- sum(uniTFM$value)
wordPerValue <- 0
wordCount <- 0
j <- 1
for(i in 1:nrow(uniTFM)){
wordCount <- wordCount + uniTFM[i, "value"]
wordCoverage <- round((wordCount/totalTermCount) * 100,2)
if (wordCoverage >= wordPercent[j]){
wordPerValue[j] <- i
j <- j+ 1
}
if(j > 9){
break
}
}

coverageDF <- as.data.frame(wordPercent)
coverageDF$uniqueWords <- wordPerValue
```

## Appendix -II (Plots)

1. Unigram Plot
```{r, eval=FALSE}
uniTFM <- getTermFreq(corpus.uniTDM, 20)

p <- ggplot(uniTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p <- p + ylab("Unigram Term Frequency in Corpus")
p <- p + xlab("Unigram Terms in Corpus") + ggtitle("Unigram Term frequency VS Term in Corpus")
p
```

2. Bigram Plot
```{r, eval=FALSE}
biTFM <- getTermFreq(corpus.biTDM, 20)
p <- ggplot(biTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("Bigram Term Frequency in Corpus")
p <- p + xlab("Bigram Terms in Corpus") + ggtitle("Bigram Term frequency VS Term in Corpus")
p
```

3. Trigram Plot
```{r, eval=FALSE}
triTFM <- getTermFreq(corpus.triTDM, 20)
p <- ggplot(triTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("Trigram Term Frequency in Corpus")
p <- p + xlab("Trigram Terms in Corpus") + ggtitle("Trigram Term frequency VS Term in Corpus")
p
```

4. 4gram Plot
```{r, eval=FALSE}
fourTFM <- getTermFreq(corpus.fourTDM, 20)
p <- ggplot(fourTFM, aes(term, value))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1), plot.title = element_text(hjust = 0.5))
p <- p + ylab("4gram Term Frequency in Corpus")
p <- p + xlab("4gram Terms in Corpus") + ggtitle("4gram Term frequency VS Term in Corpus")
p
```

5. Plot to visualize no of unique words required to cover 10% to 90% word instances.
```{r, eval=FALSE}
p <- ggplot(coverageDF, aes(wordPercent, uniqueWords)) + geom_point(alpha=0.5) + geom_line()
p <- p + geom_point(aes(x=50, y=224), size = 4, colour = "blue", alpha=0.2)
p <- p + geom_point(aes(x=90, y=4862), size = 4, colour = "red", alpha=0.2)
p <- p + geom_text(x=50, y=350, label="Word instance required for 50% coverage: 224", colour= "blue") 
p <- p + geom_text(x=65, y=4850, label="Word instance required for 90% coverage: 4862", colour= "red")
p <- p + theme(plot.title = element_text(hjust = 0.5))
p <- p + ylab("Unique Word Count")
p <- p + xlab("No of wrods in Corpus in %") + ggtitle("Unique word Count Vs No of wrods in Corpus in %")
p
```